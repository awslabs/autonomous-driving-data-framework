deploy:
  phases:
    install:
      commands:
      - npm install -g aws-cdk@2.20.0
      - pip install -r requirements.txt
    build:
      commands:
      - cdk deploy --require-approval never --progress events --app "python app.py" --outputs-file ./cdk-exports.json
      # Here we export some env vars
      - export ADDF_MODULE_METADATA=$(python -c "import json; file=open('cdk-exports.json'); print(json.load(file)['addf-${ADDF_DEPLOYMENT_NAME}-${ADDF_MODULE_NAME}']['metadata'])")
      # - Here we write env values into the dag_config.py file for use by the DAGs
      - export DAG_DIR="image_dags"
      # TODO move to proper location and update dag with path dynamically
      - wget -O $DAG_DIR/spark-dynamodb_2.12-1.1.1.jar  https://repo1.maven.org/maven2/com/audienceproject/spark-dynamodb_2.12/1.1.1/spark-dynamodb_2.12-1.1.1.jar
      - echo "ADDF_MODULE_METADATA = '${ADDF_MODULE_METADATA}'" >> $DAG_DIR/dag_config.py
      - echo "DEPLOYMENT_NAME = '${ADDF_DEPLOYMENT_NAME}'" >> $DAG_DIR/dag_config.py
      - echo "MODULE_NAME = '${ADDF_MODULE_NAME}'" >> $DAG_DIR/dag_config.py
      - echo "REGION = '${AWS_DEFAULT_REGION}'" >> $DAG_DIR/dag_config.py
      - echo "EMR_JOB_EXECUTION_ROLE = '${ADDF_PARAMETER_EMR_JOB_EXEC_ROLE}'" >> $DAG_DIR/dag_config.py
      - echo "EMR_APPLICATION_ID = '${ADDF_PARAMETER_EMR_APP_ID}'" >> $DAG_DIR/dag_config.py
      - echo "S3_SCRIPT_DIR = 's3://$ADDF_PARAMETER_DAG_BUCKET_NAME/$ADDF_PARAMETER_DAG_PATH/$ADDF_DEPLOYMENT_NAME/$ADDF_MODULE_NAME/$DAG_DIR/'" >> $DAG_DIR/dag_config.py
      - aws s3 cp --recursive $DAG_DIR/ s3://$ADDF_PARAMETER_DAG_BUCKET_NAME/$ADDF_PARAMETER_DAG_PATH/$ADDF_DEPLOYMENT_NAME/$ADDF_MODULE_NAME/$DAG_DIR/
destroy:
  phases:
    install:
      commands:
      - npm install -g aws-cdk@2.20.0
      - pip install -r requirements.txt
    build:
      commands:
      # Remove DAG files
      - aws s3 rm --recursive s3://$ADDF_PARAMETER_DAG_BUCKET_NAME/$ADDF_PARAMETER_DAG_PATH/$DAG_DIR
      - cdk destroy --force --app "python app.py"

